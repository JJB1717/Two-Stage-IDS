{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2017f5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.layers import Input, LSTM, Dropout, Dense, RepeatVector, TimeDistributed\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c5a6792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"Datasets/UNSW_NB15_training_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4a270fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define selected features\n",
    "selected_features = ['dur', 'spkts', 'dpkts', 'sbytes', \n",
    "                     'dbytes', 'sload', 'dload', \n",
    "                     'sinpkt', 'dinpkt', 'ct_srv_src', \n",
    "                     'ct_dst_ltm', 'ct_src_ltm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64166fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset based on selected features\n",
    "data_selected = data[selected_features + ['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "044de9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and labels\n",
    "X = data_selected.drop('label', axis=1)\n",
    "y = data_selected['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69da0211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deb5f05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54475c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train Autoencoder model\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "encoding_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60e3aa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Sequential([\n",
    "    Dense(32, activation='relu', input_shape=(input_dim,)),\n",
    "    Dense(encoding_dim, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(input_dim, activation=None)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da99c49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3507/3507 [==============================] - 18s 5ms/step - loss: 0.0436 - val_loss: 0.0030\n",
      "Epoch 2/10\n",
      "3507/3507 [==============================] - 15s 4ms/step - loss: 0.0077 - val_loss: 0.0182\n",
      "Epoch 3/10\n",
      "3507/3507 [==============================] - 16s 4ms/step - loss: 0.0065 - val_loss: 0.0143\n",
      "Epoch 4/10\n",
      "3507/3507 [==============================] - 15s 4ms/step - loss: 0.0042 - val_loss: 0.0762\n",
      "Epoch 5/10\n",
      "3507/3507 [==============================] - 16s 4ms/step - loss: 0.0102 - val_loss: 0.0014\n",
      "Epoch 6/10\n",
      "3507/3507 [==============================] - 15s 4ms/step - loss: 0.0046 - val_loss: 0.0031\n",
      "Epoch 7/10\n",
      "3507/3507 [==============================] - 16s 4ms/step - loss: 0.0057 - val_loss: 0.0022\n",
      "Epoch 8/10\n",
      "3507/3507 [==============================] - 16s 5ms/step - loss: 0.0043 - val_loss: 0.0012\n",
      "Epoch 9/10\n",
      "3507/3507 [==============================] - 16s 4ms/step - loss: 0.0046 - val_loss: 0.0436\n",
      "Epoch 10/10\n",
      "3507/3507 [==============================] - 16s 4ms/step - loss: 0.0030 - val_loss: 0.0061\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1bfda9d0310>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57a44b9c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n# Initialize a list to store resampled X_train and y_train for each output\\nX_train_resampled_list = []\\ny_train_resampled_list = []\\n\\n# Get the maximum number of samples after resampling\\nmax_samples = 0\\n\\n# Loop through each output variable\\nfor i in range(y_train.shape[1]):\\n    # Extract the target variable for the current output variable\\n    y_train_i = y_train.iloc[:, i]\\n    \\n    # Apply SMOTE to balance the classes for the current output variable\\n    smote = SMOTE(random_state=42)\\n    X_train_resampled_i, y_train_resampled_i = smote.fit_resample(X_train_encoded, y_train_i)\\n    \\n    # Update the maximum number of samples if needed\\n    max_samples = max(max_samples, len(X_train_resampled_i))\\n    \\n    # Append the resampled data to the list\\n    X_train_resampled_list.append(X_train_resampled_i)\\n    y_train_resampled_list.append(y_train_resampled_i)\\n\\n# Pad the resampled data for each output variable to ensure they have the same number of samples\\nX_train_resampled_padded = []\\ny_train_resampled_padded = []\\n\\nfor i in range(len(X_train_resampled_list)):\\n    X_train_resampled_padded.append(np.pad(X_train_resampled_list[i], ((0, max_samples - len(X_train_resampled_list[i])), (0, 0)), mode='constant', constant_values=0))\\n    y_train_resampled_padded.append(np.pad(y_train_resampled_list[i], (0, max_samples - len(y_train_resampled_list[i])), mode='constant', constant_values=0))\\n\\n# Concatenate the padded resampled data for all output variables\\nX_train_resampled = np.concatenate(X_train_resampled_padded, axis=1)\\ny_train_resampled = np.column_stack(y_train_resampled_padded)\\n\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Initialize a list to store resampled X_train and y_train for each output\n",
    "X_train_resampled_list = []\n",
    "y_train_resampled_list = []\n",
    "\n",
    "# Get the maximum number of samples after resampling\n",
    "max_samples = 0\n",
    "\n",
    "# Loop through each output variable\n",
    "for i in range(y_train.shape[1]):\n",
    "    # Extract the target variable for the current output variable\n",
    "    y_train_i = y_train.iloc[:, i]\n",
    "    \n",
    "    # Apply SMOTE to balance the classes for the current output variable\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled_i, y_train_resampled_i = smote.fit_resample(X_train_encoded, y_train_i)\n",
    "    \n",
    "    # Update the maximum number of samples if needed\n",
    "    max_samples = max(max_samples, len(X_train_resampled_i))\n",
    "    \n",
    "    # Append the resampled data to the list\n",
    "    X_train_resampled_list.append(X_train_resampled_i)\n",
    "    y_train_resampled_list.append(y_train_resampled_i)\n",
    "\n",
    "# Pad the resampled data for each output variable to ensure they have the same number of samples\n",
    "X_train_resampled_padded = []\n",
    "y_train_resampled_padded = []\n",
    "\n",
    "for i in range(len(X_train_resampled_list)):\n",
    "    X_train_resampled_padded.append(np.pad(X_train_resampled_list[i], ((0, max_samples - len(X_train_resampled_list[i])), (0, 0)), mode='constant', constant_values=0))\n",
    "    y_train_resampled_padded.append(np.pad(y_train_resampled_list[i], (0, max_samples - len(y_train_resampled_list[i])), mode='constant', constant_values=0))\n",
    "\n",
    "# Concatenate the padded resampled data for all output variables\n",
    "X_train_resampled = np.concatenate(X_train_resampled_padded, axis=1)\n",
    "y_train_resampled = np.column_stack(y_train_resampled_padded)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb9e9dec",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Apply SMOTE for class imbalance handling\\nsmote = SMOTE(random_state=42)\\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train_encoded, y_train)\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Apply SMOTE for class imbalance handling\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_encoded, y_train)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8c84df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4384/4384 [==============================] - 12s 3ms/step\n",
      "1096/1096 [==============================] - 3s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# Extract features using the encoder part of the trained Autoencoder\n",
    "encoder = Sequential(autoencoder.layers[:2])\n",
    "X_train_encoded = encoder.predict(X_train_scaled)\n",
    "X_test_encoded = encoder.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4cf1a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the encoded features for LSTM input\n",
    "X_train_reshaped = X_train_encoded.reshape(X_train_encoded.shape[0], 1, X_train_encoded.shape[1])\n",
    "X_test_reshaped = X_test_encoded.reshape(X_test_encoded.shape[0], 1, X_test_encoded.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29991825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM model\n",
    "lstm_model = Sequential([\n",
    "    LSTM(64, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c4b8933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2615b347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3507/3507 [==============================] - 35s 8ms/step - loss: 0.2837 - accuracy: 0.8786 - val_loss: 0.2327 - val_accuracy: 0.9019\n",
      "Epoch 2/10\n",
      "3507/3507 [==============================] - 28s 8ms/step - loss: 0.2339 - accuracy: 0.9004 - val_loss: 0.2203 - val_accuracy: 0.9049\n",
      "Epoch 3/10\n",
      "3507/3507 [==============================] - 28s 8ms/step - loss: 0.2262 - accuracy: 0.9024 - val_loss: 0.2153 - val_accuracy: 0.9060\n",
      "Epoch 4/10\n",
      "3507/3507 [==============================] - 29s 8ms/step - loss: 0.2214 - accuracy: 0.9042 - val_loss: 0.2117 - val_accuracy: 0.9078\n",
      "Epoch 5/10\n",
      "3507/3507 [==============================] - 28s 8ms/step - loss: 0.2184 - accuracy: 0.9052 - val_loss: 0.2117 - val_accuracy: 0.9060\n",
      "Epoch 6/10\n",
      "3507/3507 [==============================] - 28s 8ms/step - loss: 0.2159 - accuracy: 0.9055 - val_loss: 0.2070 - val_accuracy: 0.9088\n",
      "Epoch 7/10\n",
      "3507/3507 [==============================] - 28s 8ms/step - loss: 0.2139 - accuracy: 0.9056 - val_loss: 0.2057 - val_accuracy: 0.9084\n",
      "Epoch 8/10\n",
      "3507/3507 [==============================] - 28s 8ms/step - loss: 0.2124 - accuracy: 0.9063 - val_loss: 0.2051 - val_accuracy: 0.9079\n",
      "Epoch 9/10\n",
      "3507/3507 [==============================] - 28s 8ms/step - loss: 0.2102 - accuracy: 0.9068 - val_loss: 0.2014 - val_accuracy: 0.9102\n",
      "Epoch 10/10\n",
      "3507/3507 [==============================] - 28s 8ms/step - loss: 0.2083 - accuracy: 0.9070 - val_loss: 0.1994 - val_accuracy: 0.9109\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = lstm_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e7fa91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1096/1096 [==============================] - 5s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred_prob = lstm_model.predict(X_test_reshaped)\n",
    "y_pred_classes = (y_pred_prob > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a65d99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.79      0.84     11169\n",
      "           1       0.91      0.96      0.93     23900\n",
      "\n",
      "    accuracy                           0.91     35069\n",
      "   macro avg       0.90      0.88      0.89     35069\n",
      "weighted avg       0.91      0.91      0.90     35069\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate classification report\n",
    "class_report = classification_report(y_test, y_pred_classes)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b88641cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1096/1096 [==============================] - 5s 4ms/step - loss: 0.2050 - accuracy: 0.9062\n",
      "Test Loss: 0.2050\n",
      "Test Accuracy: 0.9062\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy = lstm_model.evaluate(X_test_reshaped, y_test)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c3dc2af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model in native Keras format\n",
    "lstm_model.save('MODELS/test.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d0422dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnn\\anaconda3_original\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model in native Keras format\n",
    "lstm_model.save('MODELS/test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2548c302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "encoder.save('MODELS/autoencoder.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecee6975",
   "metadata": {},
   "source": [
    "## TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eb0786ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test_case(test_input):\n",
    "    \n",
    "    # Convert the test input to a DataFrame\n",
    "    test_df = pd.DataFrame(test_input, index=[0])\n",
    "    \n",
    "    # Scale the test input using the same scaler used for training data\n",
    "    test_input_scaled = scaler.transform(test_df)\n",
    "    \n",
    "    # Encode the scaled test input using the encoder part of the trained autoencoder\n",
    "    test_input_encoded = encoder.predict(test_input_scaled)\n",
    "    \n",
    "    # Reshape the encoded test input to match the shape expected by the LSTM model\n",
    "    test_input_reshaped = test_input_encoded.reshape(test_input_encoded.shape[0], 1, test_input_encoded.shape[1])\n",
    "    \n",
    "    # Make predictions using the LSTM model\n",
    "    y_pred_prob = lstm_model.predict(test_input_reshaped)\n",
    "    y_pred_classes = (y_pred_prob > 0.5).astype(int)\n",
    "    \n",
    "    if y_pred_classes[0] == 1:\n",
    "        return str(y_pred_classes[0]) + \" ---> ATTACK\"\n",
    "    else:\n",
    "        return str(y_pred_classes[0]) + \" ---> NORMAL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "86301ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "[1] ---> ATTACK\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "[0] ---> NORMAL\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "[0] ---> NORMAL\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0] ---> NORMAL\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[1] ---> ATTACK\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Provided data points\n",
    "data_points = [\n",
    "    [1.830248, 12, 8, 5116, 354, 20499.95508, 1355.007568, 162.58554, 249.264578, 5, 2, 2],\n",
    "    [0.130145, 6, 2, 986, 86, 50528.25781, 2643.205566, 26.029, 0.002, 7, 5, 4],\n",
    "    [0.353716, 6, 2, 986, 86, 18591.1875, 972.531677, 70.7432, 0.006, 7, 3, 2],\n",
    "    [0.263532, 6, 2, 986, 86, 24953.32422, 1305.344238, 52.7064, 0.001, 7, 3, 2],\n",
    "    [2.315174, 10, 8, 564, 354, 1755.375488, 1071.193726, 246.328778, 311.599438, 5, 1, 3]\n",
    "]\n",
    "# ANSWER --> 1, 0, 0, 0, 1\n",
    "\n",
    "# Column names\n",
    "columns = ['dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'sload', 'dload', 'sinpkt', 'dinpkt', 'ct_srv_src', 'ct_dst_ltm', 'ct_src_ltm']\n",
    "\n",
    "# Generate test cases\n",
    "test_cases = []\n",
    "for data_point in data_points:\n",
    "    test_case = dict(zip(columns, data_point))\n",
    "    test_cases.append(test_case)\n",
    "    \n",
    "# Print test cases\n",
    "for i, test_case in enumerate(test_cases, 1):\n",
    "    #print(f\"Test Case {i}: {test_case}\")\n",
    "    print()\n",
    "    print(process_test_case(test_case))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f802e9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow version\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1706dec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.1\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "# Check Keras version\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e162dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
